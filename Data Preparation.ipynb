{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a9e436-fca3-494d-b2a0-c524247dbe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\envs\\ox\\lib\\site-packages\\pyproj\\__init__.py:91: UserWarning: Valid PROJ data directory not found. Either set the path using the environmental variable PROJ_LIB or with `pyproj.datadir.set_data_dir`.\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "import seaborn as sns \n",
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests\n",
    "import shapely\n",
    "import pickle\n",
    "import progressbar\n",
    "import geopy.distance\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import itertools\n",
    "import warnings\n",
    "import random\n",
    "import timeit\n",
    "import time\n",
    "import threading\n",
    "import functions\n",
    "from joblib import Parallel, delayed\n",
    "from functions import *\n",
    "from shapely.geometry import Point, Polygon, LineString \n",
    "from shapely.ops import nearest_points\n",
    "from shapely.ops import transform\n",
    "from shapely import geometry, ops\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "from multiprocessing import Pool\n",
    "from random import choice, randint\n",
    "from scipy.sparse import lil_matrix\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7337f9-ed5c-4f2a-9edf-429bfcaab4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read GeoJson package from Vegvesen\n",
    "ruttger_link_geom = gpd.read_file(\"Data/kartdata4.geojson\")\n",
    "ruttger_link_geom['length'] = ruttger_link_geom['geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1de6c93-94c7-44d1-8119-ff0afb76064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read OD Nodes and All current ChargingStationsNodes\n",
    "OD = pd.read_csv(\"Data/OD_new.csv\", encoding=\"Cp1252\")\n",
    "OD = OD.drop('Unnamed: 0', axis=1)\n",
    "# OD_test = pd.read_csv(\"Data/OD_test.csv\", encoding=\"iso8859_10\")\n",
    "# OD_test = OD_test.drop('Unnamed: 0', axis=1)\n",
    "CS = pd.read_csv(\"Data/AllCurrentCS.csv\", encoding=\"Cp1252\")\n",
    "CS = CS.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddff99a-362f-4efc-a68b-4663f1f82e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform project into lat-long coordinate system (ESPG:4326)\n",
    "project = pyproj.Transformer.from_proj(\n",
    "    pyproj.Proj(init='epsg:8687'), # source coordinate system\n",
    "    pyproj.Proj(init='epsg:4326')) # destination coordinate system\n",
    "\n",
    "ruttger_link_geom['geometry'] = ruttger_link_geom['geometry'].apply(lambda x: transform(project.transform, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27bef3-34f3-4b88-ba72-d184c541e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = functions.build_network_data(ruttger_link_geom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5402c2-d12b-4745-9f6e-eeb8f0569539",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_attrs = {'crs': 'epsg:4326', 'simplified': False}\n",
    "G = ox.graph_from_gdfs(nodes, edges.drop('key', axis = 1), graph_attrs = graph_attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f620a4-39c5-49f6-bb25-e3a9d9e956df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep mainland, irrelevant as O-D Nodes are applied.\n",
    "start_node = ox.get_nearest_node(G, (59.9098, 10.7146)) \n",
    "F = G.subgraph(nx.shortest_path(G.to_undirected(), start_node)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1878a-c306-4174-98a0-2deb5bdf78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = F.copy()\n",
    "H = H.to_undirected()\n",
    "# nx.write_gpickle(H, \"Data/unsimp.gpickle\") #Save sunplified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc5b63-fa4a-4e22-bfcd-b05f711f1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve elevation of each node and grade of edges (You'll need your own Google Cloud Console API for this)\n",
    "GOOGLE_API = \"Insert API Key\"\n",
    "H_elev = ox.elevation.add_node_elevations_google(F, api_key= GOOGLE_API)\n",
    "H_elev = ox.elevation.add_edge_grades(H_elev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0c76c-c93a-46b9-9b27-a7a3de84ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = H_elev.copy()\n",
    "for i in H.edges:\n",
    "    grade = H.edges[i]['grade']\n",
    "    if grade == float('inf'):\n",
    "        # print(\"grade is inf\")\n",
    "        H.edges[i]['grade'] = 0\n",
    "    if grade == float('-inf'):\n",
    "        # print(\"grade is -inf\")\n",
    "        H.edges[i]['grade'] = 0\n",
    "    if np.isnan(grade):\n",
    "        # print(\"grade is nan\")\n",
    "        H.edges[i]['grade'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3dbb7-1a4b-4bc3-b10c-f8ca3a13e840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert roadclass to int, because simplification appends it to list\n",
    "for i in H.edges:\n",
    "    data = H.edges[i]\n",
    "    roadclass = data['funcroadclass']\n",
    "    if type(roadclass) == list:\n",
    "        # Keep minimum roadclass\n",
    "        roadclass = min(roadclass)\n",
    "    data['funcroadclass'] = roadclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd068c4-b500-49ee-89cd-81c0efaecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(H, \"Data/H.gpickle\") #Save unsimplified network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d2c695-bd1d-4309-91ed-b3f128acbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get nearest node to each O-D location\n",
    "lat = list(OD.lat)\n",
    "lon = list(OD.lon)\n",
    "ODs= ox.distance.get_nearest_nodes(H, lon, lat)\n",
    "OD['node'] = ODs\n",
    "\n",
    "#Get nearest node to each CS\n",
    "latA = list(CS.Latitude)\n",
    "lonA = list(CS.Longitude)\n",
    "AllCSnodes = ox.distance.get_nearest_nodes(H, lonA, latA)\n",
    "CS['node']=AllCSnodes\n",
    "AllCSnodes1 = list(CS.node)\n",
    "#Create combinations of O-D pairs. Used for several functions later on\n",
    "combos = combinations(ODs,2)\n",
    "OD_pairs = []\n",
    "for i in combos:\n",
    "    OD_pairs.append(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb8223-6d8d-4b4c-977b-aebf8790a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_attributes(cumlist):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes a list of network nodes and merge their attributes as the different attributes need to be treated and formatted differently. This specifically applies for LineString.\n",
    "    \n",
    "    Input:\n",
    "    cumlist: Cummulative list of nodes\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cumlength = len(cumlist)\n",
    "    path_attributes = dict()\n",
    "    attrs_to_sum = {\"length\", \"drivetime\",}\n",
    "    attrs_to_set = {'oneway', 'ref', 'name', 'funcroadclass', 'roadclass'}\n",
    "    dummy_attrs = {'isFerry', 'isTunnel', 'isBridge'}\n",
    "    nodes_to_remove = []\n",
    "    \n",
    "    for i in range(cumlength-1):\n",
    "        u = cumlist[i]\n",
    "        v = cumlist[i+1]\n",
    "        edge_data = H.edges[u, v,0]\n",
    "\n",
    "        for attr in edge_data:\n",
    "            if attr in path_attributes:\n",
    "            # if this key already exists in the dict, append it to the value list\n",
    "                path_attributes[attr].append(edge_data[attr])\n",
    "            else:\n",
    "            # if this key doesn't already exist, set the value to a list containing the one value\n",
    "                path_attributes[attr] = [edge_data[attr]]\n",
    "\n",
    "    for attr in path_attributes:\n",
    "        if attr == 'grade' or attr == 'length_weight':\n",
    "            path_attributes[attr] = list(path_attributes[attr]) \n",
    "        elif attr in dummy_attrs:\n",
    "            #If attribute is isFerry, isBridge og isTunnel, returns the value for first and last edge as these are candidate locations.\n",
    "            if type(path_attributes[attr][0]) == list and type(path_attributes[attr][-1]) == list:\n",
    "                fromnode = path_attributes[attr][0][0]\n",
    "                tonode = path_attributes[attr][-1][-1]\n",
    "                \n",
    "            elif type(path_attributes[attr][0]) != list and type(path_attributes[attr][-1]) != list:\n",
    "                fromnode = path_attributes[attr][0]\n",
    "                tonode = path_attributes[attr][-1]\n",
    "                \n",
    "            elif type(path_attributes[attr][0]) == list and type(path_attributes[attr][-1]) != list:\n",
    "                fromnode = path_attributes[attr][0][0]\n",
    "                tonode = path_attributes[attr][-1]\n",
    "                \n",
    "            elif type(path_attributes[attr][0]) != list and type(path_attributes[attr][-1]) == list:\n",
    "                fromnode = path_attributes[attr][0]\n",
    "                tonode = path_attributes[attr][-1][-1]\n",
    "            path_attributes[attr] = list((fromnode, tonode))\n",
    "\n",
    "        elif attr in attrs_to_sum:\n",
    "            # if this attribute must be summed, sum it now\n",
    "            path_attributes[attr] = sum(path_attributes[attr])\n",
    "\n",
    "        elif attr == 'id':\n",
    "            path_attributes[attr] = list((cumlist[0],cumlist[-1]))\n",
    "\n",
    "        \n",
    "        elif attr == 'geometry':\n",
    "                path_attributes[attr] = ops.linemerge(path_attributes[attr])\n",
    "                \n",
    "        elif attr == 'oneway':\n",
    "            path_attributes[attr] = 'False'\n",
    "\n",
    "\n",
    "        else:\n",
    "        # otherwise, if there are multiple values, keep one of each\n",
    "            try:\n",
    "                path_attributes[attr] = list(set(path_attributes[attr]))\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    path_attributes[attr] = list(set(path_attributes[attr][0]))\n",
    "                except:\n",
    "                    path_attributes[attr] = path_attributes[attr]\n",
    "            \n",
    "    path_attributes['artificial'] = 1\n",
    "    return path_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec59cee-7008-452a-9516-b5eeba521391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shorten_edges_v2(G, OD, cutoff):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simplifies the edges in the network in order to make the runtimes of the GA/greedy substitution feassible. Depending on the threshold value, the function will remove all nodes/edges within a certain range, store and merge\n",
    "    their attributes, before creating a new edge between the first and last node in the specified range. If threshol is for example 50000, the function will remove all nodes except the first and last node for every 50th km and \n",
    "    create a new edge between the first and last node. This i then repeated for each path between each OD pair.\n",
    "    \n",
    "    There are also certain statements to make sure that the same nodes are used if several paths intersect. This usually happens when multiple routes shares the same path. \n",
    "    \n",
    "    Input:\n",
    "    G: Networkx Multigraph\n",
    "    OD: CSV file with coordinates of each OD location in Norway\n",
    "    Threshold: Cutoff in meters at which edges at merged. \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #Create flat list of O-D pairs\n",
    "    OD_pairs_flat = [item for sublist in OD_pairs for item in sublist]\n",
    "    \n",
    "    #Create empty MultiGraph\n",
    "    B = nx.MultiGraph(crs='epsg:4326')\n",
    "    \n",
    "    #Initialize progressbar to monitor progress of function while running.\n",
    "    maxval = len(OD_pairs)\n",
    "    bar    = progressbar.ProgressBar(maxval=maxval, \\\n",
    "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "    counter=0\n",
    "    bar.start()\n",
    "\n",
    "    UsedNodes = []\n",
    "    for i in OD_pairs:\n",
    "        fromOD         = i[0]\n",
    "        toOD           = i[1]\n",
    "        path           = nx.shortest_path(H, fromOD, toOD, 'length')\n",
    "        pathlength     = len(path)\n",
    "        lengthofpath   = nx.shortest_path_length(H, fromOD, toOD, 'length')\n",
    "        cumlength      = 0\n",
    "        cumlist        = []\n",
    "        cutoff         = cutoff\n",
    "\n",
    "\n",
    "        #for each in edge in path[i]\n",
    "        for i in range(pathlength-1):\n",
    "            fromnode = path[i]\n",
    "            tonode = path[i+1]\n",
    "            templength = H.edges[fromnode, tonode, 0]['length']\n",
    "\n",
    "            cumlist.append(fromnode)\n",
    "            cumlist.append(tonode)\n",
    "            cumlength += templength\n",
    "\n",
    "            #First check if tonode already exists in B. If it does, it means that a previously created path has used same route. \n",
    "            #In this case we want to use existing nodes on path rather than creating new ones.\n",
    "            if tonode in UsedNodes:\n",
    "                #If edge does not exist, then add it\n",
    "                if B.has_edge(cumlist[0], cumlist[-1]) is False:\n",
    "                    cumlist = list(dict.fromkeys(cumlist))\n",
    "                    #Get attributes for nodes and edge to add\n",
    "                    fnodeattr = H.nodes[cumlist[0]]\n",
    "                    tnodeattr = H.nodes[cumlist[-1]]\n",
    "                    edgeattr = get_edge_attributes(cumlist)\n",
    "\n",
    "                    #Add nodes and edge\n",
    "                    B.add_node(cumlist[0], **fnodeattr)\n",
    "                    B.add_node(cumlist[-1], **tnodeattr)\n",
    "                    B.add_edge(cumlist[0], cumlist[-1], **edgeattr)\n",
    "\n",
    "                    #Added nodes are appended to all UsedNodes\n",
    "                    UsedNodes.append(cumlist[0])\n",
    "                    UsedNodes.append(cumlist[-1])\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "\n",
    "\n",
    "                #Else, skip this part, but empty cummulative path and length\n",
    "                else:\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "                    pass\n",
    "                \n",
    "            elif tonode in AllCSnodes: #if next node is a CS node\n",
    "                #If edge does not exist, then add it\n",
    "                if B.has_edge(cumlist[0], cumlist[-1]) is False:\n",
    "                    cumlist = list(dict.fromkeys(cumlist))\n",
    "                    #Get attributes for nodes and edge to add\n",
    "                    fnodeattr = H.nodes[cumlist[0]]\n",
    "                    tnodeattr = H.nodes[cumlist[-1]]\n",
    "                    edgeattr = get_edge_attributes(cumlist)\n",
    "\n",
    "                    #Add nodes and edge\n",
    "                    B.add_node(cumlist[0], **fnodeattr)\n",
    "                    B.add_node(cumlist[-1], **tnodeattr)\n",
    "                    B.add_edge(cumlist[0], cumlist[-1], **edgeattr)\n",
    "\n",
    "                    #Added nodes are appended to all UsedNodes\n",
    "                    UsedNodes.append(cumlist[0])\n",
    "                    UsedNodes.append(cumlist[-1])\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "\n",
    "\n",
    "                #Else, skip this part, but empty cummulative path and length\n",
    "                else:\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "                    pass\n",
    "                \n",
    "\n",
    "            #Next, check if tonode is an OD node. If it is we will create edge from start of cumlist to the OD node.\n",
    "            elif tonode in OD_pairs_flat:\n",
    "                #If edge does not exist, then add it\n",
    "                if B.has_edge(cumlist[0], cumlist[-1]) is False:\n",
    "                    cumlist = list(dict.fromkeys(cumlist))\n",
    "                    #Get attributes for nodes and edge to add\n",
    "                    fnodeattr = H.nodes[cumlist[0]]\n",
    "                    tnodeattr = H.nodes[cumlist[-1]]\n",
    "                    edgeattr = get_edge_attributes(cumlist)\n",
    "\n",
    "                    #Add nodes and edge\n",
    "                    B.add_node(cumlist[0], **fnodeattr)\n",
    "                    B.add_node(cumlist[-1], **tnodeattr)\n",
    "                    B.add_edge(cumlist[0], cumlist[-1], **edgeattr)\n",
    "\n",
    "                    UsedNodes.append(cumlist[0])\n",
    "                    UsedNodes.append(cumlist[-1])\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "                #Else, skip this part, but empty cummulative path and length\n",
    "                else:\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "                    pass\n",
    "\n",
    "            #Next, check if the cummulative length in path is higher than the set threshold. If it is we will create a new edge in B\n",
    "            #Can consider adding an if inside here that check if there is a node within a certain radius, and skip if it is.\n",
    "            #This could prevent that nodes on completely different paths are very close or that a node is set very close to an OD node. \n",
    "            #Would then find last element in cumlist in H and get lat,lon, then use ox nearest node to find nearest node in B and finally use geopy to find distance between them. \n",
    "\n",
    "            elif cumlength >= threshold:            \n",
    "\n",
    "\n",
    "                if B.has_edge(cumlist[0], cumlist[-1]) is False:\n",
    "                    #Remove duplicates from cumlist\n",
    "                    cumlist = list(dict.fromkeys(cumlist))\n",
    "                    distanceOD = 100000\n",
    "                    #Before we add new edge, check if destination node is within threshold/2\n",
    "                    lat = H.nodes[cumlist[-1]]['y']\n",
    "                    lon = H.nodes[cumlist[-1]]['x']\n",
    "                    latlon = (lat,lon)\n",
    "                    latdestination = H.nodes[toOD]['y']\n",
    "                    londestination = H.nodes[toOD]['x']\n",
    "\n",
    "                    #Also check if there is an already created node nearby\n",
    "                    try:\n",
    "                        potentialnode = ox.get_nearest_node(B, latlon)\n",
    "                        clatclon = (latdestination,londestination)\n",
    "                        distanceOD = (geopy.distance.geodesic(latlon,clatclon).m)\n",
    "\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                    # clatclon = (latdestination,londestination)\n",
    "                    # distanceOD = (geopy.distance.geodesic(latlon,clatclon).m)\n",
    "                    # distancepath = geopy.distance.geodesic(latlon, platplon).m\n",
    "\n",
    "                    if distanceOD <= threshold/2:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #Get attributes for nodes and edge to add\n",
    "                        fnodeattr = H.nodes[cumlist[0]]\n",
    "                        tnodeattr = H.nodes[cumlist[-1]]\n",
    "                        edgeattr = get_edge_attributes(cumlist)\n",
    "\n",
    "                        #Add nodes and edge\n",
    "                        B.add_node(cumlist[0], **fnodeattr)\n",
    "                        B.add_node(cumlist[-1], **tnodeattr)\n",
    "                        B.add_edge(cumlist[0], cumlist[-1], **edgeattr)\n",
    "\n",
    "                        UsedNodes.append(cumlist[0])\n",
    "                        UsedNodes.append(cumlist[-1])                \n",
    "                        cumlist = []\n",
    "                        cumlength = 0\n",
    "\n",
    "                #Else, skip this part, but empty cummulative path and length\n",
    "                else:\n",
    "                    cumlist = []\n",
    "                    cumlength = 0\n",
    "                    pass\n",
    "\n",
    "        bar.update(counter)\n",
    "        counter+=1\n",
    "\n",
    "    bar.finish()\n",
    "\n",
    "    #Set OD=1 for all O-D nodes and CS=1 for all current CS nodes. \n",
    "    nx.set_node_attributes(B, 0, 'OD')\n",
    "    nx.set_node_attributes(B, 0, 'CS')\n",
    "\n",
    "    attrs = {}\n",
    "    for i in OD_pairs_flat:\n",
    "        O = i\n",
    "        attrs[i] = {'OD':1}\n",
    "    nx.set_node_attributes(B, attrs)\n",
    "    \n",
    "    attrs = {}\n",
    "    for i in AllCSnodes:\n",
    "        O = i\n",
    "        attrs[i] = {'CS':1}\n",
    "    nx.set_node_attributes(B, attrs)\n",
    "    \n",
    "    \n",
    "    print(\"Complete!\")\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e5aa8-f3a7-4a03-ab5e-52dbaa782985",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = shorten_edges_v2(H, OD, 10000) #Select cut-off value\n",
    "nx.write_gpickle(B,\"Data/B10000.gpickle\")\n",
    "B = shorten_edges_v2(H, OD, 2000)\n",
    "nx.write_gpickle(B,\"Data/B2000.gpickle\")\n",
    "B = shorten_edges_v2(H, OD, 5000)\n",
    "nx.write_gpickle(B,\"Data/B5000.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac49dd8-69f8-4103-8d7a-b972f339b662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86369e39-a13a-4be9-8b92-1de9d2f0863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic figure of network\n",
    "node_color = []\n",
    "for i in B.nodes:\n",
    "    if B.nodes[i]['CS']==1:\n",
    "            node_color.append('red')\n",
    "    else:\n",
    "        node_color.append('c')\n",
    "ox.plot_graph(B, node_color=node_color, node_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e229891-18a7-4eb6-bfa3-5ff3971a3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten lists of lists for 'grade' and length_weight. Must be done because the simplification process creates list of lists for merged edges\n",
    "#Run entire loop 10 times to ensure that all lists are flattended.\n",
    "for i in range(10):\n",
    "    for i in B.edges:\n",
    "        flat_list_length_weight = []\n",
    "        flat_list_grade = []\n",
    "        if type(B.edges[i]['length_weight']) == list:\n",
    "            for sublist in B.edges[i]['length_weight']:\n",
    "                try:\n",
    "                    for item in sublist:\n",
    "                        flat_list_length_weight.append(item)\n",
    "                except TypeError:\n",
    "                    flat_list_length_weight.append(sublist)\n",
    "            B.edges[i]['length_weight'] = flat_list_length_weight\n",
    "        else:\n",
    "            pass\n",
    "        if type(B.edges[i]['grade']) == list:\n",
    "            for sublist in B.edges[i]['grade']:\n",
    "                try:\n",
    "                    for item in sublist:\n",
    "                        flat_list_grade.append(item)\n",
    "                except TypeError:\n",
    "                    flat_list_grade.append(sublist)\n",
    "            B.edges[i]['grade'] = flat_list_grade\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acc279e7-d378-4e92-a763-4ba43113e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eff_lengths(edge):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the effective length of an edge by using length weights and road gradient\n",
    "    \"\"\"\n",
    "\n",
    "    startnode = edge[0]\n",
    "    endnode = edge[1]\n",
    "    const = 0.372\n",
    "    grade_intervals = [-0.09, -0.07, -0.05, -0.03, -0.01, 0.01, 0.03, 0.05, 0.07, 0.09, 0.11]\n",
    "    increased_consumption = [-0.332, -0.217, -0.148, -0.121, -0.073, 0.085, 0.152, 0.203, 0.306, 0.358, 0.552]\n",
    "    \n",
    "    iterator = len(B.edges[edge]['length_weight'])\n",
    "    \n",
    "    #If the direction of the edge is the default fromnode -> tonode, we set a multiplier to 1, meaning that the edge gradients will be default:\n",
    "    if B.nodes[startnode]['elevation'] < B.nodes[endnode]['elevation']: \n",
    "        direction = 1\n",
    "    #If direction is reversed, meaning that the vehicle is returning to the origin node, we reverse the gradients\n",
    "    else:\n",
    "        direction = -1\n",
    "    eff_length_trueway = 0\n",
    "        \n",
    "    for i in range(iterator):\n",
    "        edge_grad = B.edges[edge]['grade'][i]*(direction)\n",
    "        length_weight = B.edges[edge]['length_weight'][i]\n",
    "\n",
    "        if edge_grad < grade_intervals[0]: #less than -9%\n",
    "            diff = const + increased_consumption[0]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[0] < edge_grad <= grade_intervals[1]: #-9% to -7%\n",
    "            diff = const + increased_consumption[1]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[1] < edge_grad <= grade_intervals[2]: #-7% to -5%\n",
    "            diff = const + increased_consumption[2]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[2] < edge_grad <= grade_intervals[3]: #-5% to -3%\n",
    "            diff = const + increased_consumption[3]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[3] <= edge_grad <= grade_intervals[4]: #-3% to -1%\n",
    "            diff = const + increased_consumption[4]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[5] <= edge_grad <= grade_intervals[6]: #1% to #3%\n",
    "            diff = const + increased_consumption[5]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[6] < edge_grad <= grade_intervals[7]: #3% to 5%\n",
    "            diff = const + increased_consumption[6]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[7] < edge_grad <= grade_intervals[8]: #5% to 7%\n",
    "            diff = const + increased_consumption[7]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[8] < edge_grad <= grade_intervals[9]: #7% to 9%\n",
    "            diff = const + increased_consumption[8]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif grade_intervals[9] < edge_grad <= grade_intervals[10]: #9% to 11%\n",
    "            diff = const + increased_consumption[9]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "        elif edge_grad > grade_intervals[10]: #more then 11%\n",
    "            diff = const + increased_consumption[0]\n",
    "            eff_length_trueway += length_weight*(diff/const)\n",
    "\n",
    "\n",
    "        else: #If grade is between -1% and 1%\n",
    "            eff_length_trueway += length_weight\n",
    "            diff = 0\n",
    "     \n",
    "    return eff_length_trueway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29b4a0e2-2d74-439c-867d-f0e4dcab6b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create df of all effective lengths for both directions\n",
    "#Also create df containing total range between OD pairs both directions\n",
    "alledges = []\n",
    "true = []\n",
    "for i in B.edges:\n",
    "    startnode = i[0]\n",
    "    endnode = i[1]    \n",
    "    trueway = [startnode,endnode,0]\n",
    "    alledges.append(trueway)\n",
    "    true.append(\"true\")\n",
    "    revway = [endnode, startnode,0]\n",
    "    alledges.append(revway)\n",
    "    true.append(\"false\")\n",
    "    \n",
    "edge = []\n",
    "eff_len = []\n",
    "lengths = pd.DataFrame(columns = ['fromnode','tonode', 'eff_len'])\n",
    "for i in alledges:\n",
    "    startnode = i[0]\n",
    "    endnode = i[1]\n",
    "    bothnodes = startnode,endnode\n",
    "    \n",
    "    edge.append(list(i))\n",
    "    result = calculate_eff_lengths(list(i))\n",
    "    eff_len.append(result)\n",
    "    \n",
    "    if list(bothnodes) == B.edges[i]['id']:\n",
    "        B.edges[i]['eff_len_t'] = result\n",
    "    else:\n",
    "        B.edges[i]['eff_len_f'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a887a7a-c48e-49e5-a7c6-153e5dcb9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df\n",
    "\n",
    "#First add w and w_s to each node\n",
    "for count, value in enumerate(OD['node']):\n",
    "    B.nodes[value]['w']   = OD.iloc[count]['w']\n",
    "    B.nodes[value]['w_s'] = OD.iloc[count]['w_s']\n",
    "    \n",
    "#Next, create df\n",
    "df = pd.DataFrame(columns = ['fromnode', 'tonode', 'drivetime', 'length', 'weight', 'weight_s', 'path'])\n",
    "\n",
    "for i in OD_pairs:\n",
    "    tempfrom, tempto = i\n",
    "    \n",
    "    path       = nx.shortest_path(B, tempfrom, tempto, 'length')\n",
    "    pathlength = nx.shortest_path_length(B, tempfrom, tempto, 'length')\n",
    "    pathlen    = len(path)\n",
    "    weight     = float(OD.loc[OD['node']==tempfrom]['w'])*float(OD.loc[OD['node']==tempto]['w'])\n",
    "    weight_s   = float(OD.loc[OD['node']==tempfrom]['w_s'])*float(OD.loc[OD['node']==tempto]['w_s'])\n",
    "    drivetime  = 0\n",
    "    \n",
    "    for i in range(pathlen-1):\n",
    "        drivetime += B.edges[path[i],path[i+1],0]['drivetime']\n",
    "    \n",
    "    temp = tempfrom, tempto, drivetime, pathlength, weight, weight_s, path\n",
    "    seriestoappend = pd.Series(temp, index=df.columns)\n",
    "    df = df.append(seriestoappend, ignore_index=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f386817-c027-4e14-87bf-664fd736f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, remove faulty added nodes and edges. These would only act as noise when visualising.\n",
    "\n",
    "for i in B.edges:\n",
    "    B.edges[i]['used'] =0\n",
    "\n",
    "for i in B.nodes:\n",
    "    B.nodes[i]['used'] = 0\n",
    "    \n",
    "for i in df.index:\n",
    "    path = df.iloc[i]['path']\n",
    "    for p in range(len(path)):\n",
    "        try:\n",
    "            a = path[p]\n",
    "            b = path[p+1]\n",
    "            B.edges[a,b,0]['used'] = 1\n",
    "            B.nodes[a]['used']=1\n",
    "            B.nodes[b]['used']=1\n",
    "        except:\n",
    "            pass    \n",
    "unusedNodes = [] #List of unused nodes\n",
    "for i in B.nodes:\n",
    "    if B.nodes[i]['used'] == 0:\n",
    "        unusedNodes.append(i)   \n",
    "        \n",
    "unusedEdges = []\n",
    "for i in B.edges:\n",
    "    if B.edges[i]['used'] ==0:\n",
    "        unusedEdges.append(i)\n",
    "\n",
    "#Remove all unused nodes\n",
    "for n in unusedNodes:\n",
    "    B.remove_node(n)\n",
    "\n",
    "#Remove all unused edges\n",
    "for e in unusedEdges:\n",
    "    a = e[0]\n",
    "    b = e[1]\n",
    "    try:\n",
    "        B.remove_edge(a,b)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddeb718a-457b-4484-a4c5-25cd50f40180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dictionary of effective lengths for each OD pair in both directions. Store lengths in dictionary because this is more time efficient.\n",
    "keys = []\n",
    "vals = []\n",
    "for i in range(len(df)):\n",
    "    fromnode = df.iloc[i]['fromnode']\n",
    "    tonode = df.iloc[i]['tonode']\n",
    "    path = df.iloc[i]['path']\n",
    "    eff_len_default =0\n",
    "    eff_len_false = 0\n",
    "    for i in range(len(path)):\n",
    "        try:\n",
    "            edge = [path[i],path[i+1],0]\n",
    "            eff_edge_len = calculate_eff_lengths(edge)\n",
    "            eff_len_default += eff_edge_len\n",
    "        except IndexError:\n",
    "            pass\n",
    "    path.reverse()\n",
    "    for i in range(len(path)):\n",
    "        try:\n",
    "            eff_edge_len = calculate_eff_lengths(edge)\n",
    "            eff_len_false += eff_edge_len\n",
    "        except IndexError:\n",
    "            pass\n",
    "    fromto = fromnode, tonode\n",
    "    tofrom = tonode,fromnode\n",
    "    keys.append(fromto)\n",
    "    keys.append(tofrom)\n",
    "    vals.append(eff_len_default)\n",
    "    vals.append(eff_len_false)\n",
    "zip_iter = zip(keys,vals)\n",
    "lengths_dict = dict(zip_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "597b4490-06c2-48f1-88e6-ad15997676f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define gravity model that estimates the flow between each O-D pair\n",
    "def GravityModel(master_df):\n",
    "    flow=[]\n",
    "    master_df['flow']=0\n",
    "    for i in master_df.index:\n",
    "        GC=(0.75*(master_df['length'][i])+0.5*master_df['drivetime'][i])\n",
    "        flow.append(master_df['weight'][i]/GC)\n",
    "    \n",
    "    tot_flow=sum(flow)\n",
    "    for i in master_df.index:\n",
    "        master_df['flow'][i]=flow[i]/tot_flow\n",
    "GravityModel(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62e9a7-701d-4f9a-a225-f042bb91d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add rest of CS stations that can be localized within a 10km distance.\n",
    "bnodes = []\n",
    "name = []\n",
    "Latitude = []\n",
    "Longitude = []\n",
    "node = []\n",
    "for count, i in enumerate(list(AllCSnodes)):\n",
    "    # if i not in B.nodes:\n",
    "    try:\n",
    "        latCS = float(CS[CS['node']==i]['Latitude'])\n",
    "        lonCS = float(CS[CS['node']==i]['Longitude'])\n",
    "        tempname = CS[CS['node']==i]['Name'][count]\n",
    "    except TypeError:\n",
    "        latCS = list(CS[CS['node']==i]['Latitude'])[0]\n",
    "        lonCS = list(CS[CS['node']==i]['Longitude'])[0]\n",
    "        tempname = CS[CS['node']==i]['Name'][count]\n",
    "    potentialnode = ox.nearest_nodes(B, lonCS, latCS)\n",
    "    lat = B.nodes[potentialnode]['y']\n",
    "    lon = B.nodes[potentialnode]['x']\n",
    "    dist = geopy.distance.geodesic((lat,lon),(latCS, lonCS)).m\n",
    "    if dist <10000 and B.nodes[potentialnode]['CS']==0:\n",
    "        name.append(tempname)\n",
    "        Latitude.append(latCS)\n",
    "        Longitude.append(lonCS)\n",
    "        node.append(potentialnode)\n",
    "        B.nodes[potentialnode]['CS']=1 \n",
    "d = {'Name':name, 'Latitude':Latitude, 'Longitude':Longitude, 'node':node}\n",
    "CS_simplified = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c7e7e-a2f5-41f2-b0f7-564285c31d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally export all items\n",
    "nx.write_gpickle(B, \"Data\\B10000-CS2.gpickle\")\n",
    "nx.write_gpickle(H, \"Data\\H.gpickle\")\n",
    "df.to_pickle(\"Data/flows.csv\")\n",
    "df.to_pickle(\"Data/CS_simplified.csv\")\n",
    "with open('Data/lengths.pickle', 'wb') as handle:\n",
    "    pickle.dump(lengths_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('Data/AllCSnodes.pickle', 'wb') as handle:\n",
    "    pickle.dump(AllCSnodes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ox)",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
